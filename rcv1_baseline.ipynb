{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44e2812f",
   "metadata": {},
   "source": [
    "# RCV1 Text Classification Baseline\n",
    "\n",
    "This notebook implements a strong baseline for the Reuters Corpus Volume I (RCV1) multi-label text classification task. The dataset contains over 800,000 news stories with multiple labels in three taxonomies: topics, industries, and regions.\n",
    "\n",
    "We'll focus on creating an efficient and high-performing baseline using modern approaches in scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c10e0a7",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffcb08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import sparse\n",
    "from sklearn.datasets import fetch_rcv1\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Transformer-related imports\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda import is_available\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d9da4c",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b9235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "print('Loading RCV1 dataset...')\n",
    "rcv1 = fetch_rcv1()\n",
    "\n",
    "# Print basic information\n",
    "print('\\nDataset Features:')\n",
    "print(f'Number of samples: {rcv1.data.shape[0]}')\n",
    "print(f'Number of features: {rcv1.data.shape[1]}')\n",
    "print(f'Number of topics: {rcv1.target.shape[1]}')\n",
    "print(f'Data type: {type(rcv1.data)}')\n",
    "print(f'Target type: {type(rcv1.target)}')\n",
    "\n",
    "# Show sparsity\n",
    "print('\\nSparsity statistics:')\n",
    "print(f'Data sparsity: {rcv1.data.nnz / (rcv1.data.shape[0] * rcv1.data.shape[1]):.4%}')\n",
    "print(f'Target sparsity: {rcv1.target.nnz / (rcv1.target.shape[0] * rcv1.target.shape[1]):.4%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6744ab40",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data\n",
    "\n",
    "The RCV1 dataset comes already preprocessed with TF-IDF features, but we'll normalize them to ensure better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c8131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features\n",
    "X = normalize(rcv1.data, norm='l2', copy=False)\n",
    "y = rcv1.target\n",
    "\n",
    "# Convert to CSR format for efficiency\n",
    "X = X.tocsr()\n",
    "y = y.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6b43ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCV1Dataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension added by tokenizer\n",
    "        item = {\n",
    "            key: val.squeeze(0) for key, val in encoding.items()\n",
    "        }\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.FloatTensor(self.labels[idx])\n",
    "            \n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e8dfd6",
   "metadata": {},
   "source": [
    "## 4. Split Dataset\n",
    "\n",
    "We'll use the official chronological split provided by LYRL2004:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50225f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the first 23,149 documents as training set (as per LYRL2004 split)\n",
    "train_size = 23149\n",
    "\n",
    "# Split the data\n",
    "X_train = X[:train_size]\n",
    "X_test = X[train_size:]\n",
    "y_train = y[:train_size]\n",
    "y_test = y[train_size:]\n",
    "\n",
    "# Create a validation set from training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print('Dataset splits:')\n",
    "print(f'Training set: {X_train.shape[0]} samples')\n",
    "print(f'Validation set: {X_val.shape[0]} samples')\n",
    "print(f'Test set: {X_test.shape[0]} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed56a195",
   "metadata": {},
   "source": [
    "## 5. Define Baseline Model\n",
    "\n",
    "For our baseline, we'll use LogisticRegression with the following optimizations:\n",
    "1. L2 regularization for better generalization\n",
    "2. 'saga' solver for efficient optimization\n",
    "3. Class weights to handle imbalance\n",
    "4. MultiOutputClassifier for parallel training on multiple labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80645e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the base classifier\n",
    "base_clf = LogisticRegression(\n",
    "    C=4.0,  # Reduced regularization strength for better recall\n",
    "    solver='saga',  # Efficient solver for large-scale data\n",
    "    penalty='l2',\n",
    "    max_iter=100,\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Wrap it in MultiOutputClassifier for multi-label classification\n",
    "model = MultiOutputClassifier(base_clf, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTForMultiLabelClassification(nn.Module):\n",
    "    def __init__(self, num_labels, model_name='distilbert-base-uncased'):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        pooled_output = outputs[0][:, 0]  # Get CLS token output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27750ac6",
   "metadata": {},
   "source": [
    "## 6. Train Model\n",
    "\n",
    "We'll train the model and monitor the progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47169852",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training the model...')\n",
    "model.fit(X_train, y_train.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db1044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc='Training')\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "print('Loading DistilBERT tokenizer and model...')\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = BERTForMultiLabelClassification(num_labels=y_train.shape[1]).to(device)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = RCV1Dataset(\n",
    "    texts=[' '.join(rcv1.target_names[i] for i in doc.nonzero()[1]) for doc in X_train],\n",
    "    labels=y_train.toarray(),\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "val_dataset = RCV1Dataset(\n",
    "    texts=[' '.join(rcv1.target_names[i] for i in doc.nonzero()[1]) for doc in X_val],\n",
    "    labels=y_val.toarray(),\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Training settings\n",
    "num_epochs = 3\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = BCEWithLogitsLoss()\n",
    "\n",
    "# Training loop\n",
    "print('Training BERT model...')\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\nEpoch {epoch + 1}/{num_epochs}')\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer, criterion, device)\n",
    "    print(f'Average training loss: {train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcae5162",
   "metadata": {},
   "source": [
    "## 7. Evaluate Model\n",
    "\n",
    "We'll evaluate the model using metrics appropriate for multi-label classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310f5f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(y_true, y_pred, dataset_name):\n",
    "    \"\"\"Evaluate predictions using multiple metrics\"\"\"\n",
    "    # Convert sparse matrix to dense array if needed\n",
    "    if sparse.issparse(y_true):\n",
    "        y_true = y_true.toarray()\n",
    "    \n",
    "    metrics = {\n",
    "        'Micro F1': f1_score(y_true, y_pred, average='micro'),\n",
    "        'Macro F1': f1_score(y_true, y_pred, average='macro'),\n",
    "        'Weighted F1': f1_score(y_true, y_pred, average='weighted'),\n",
    "        'Samples F1': f1_score(y_true, y_pred, average='samples'),\n",
    "        'Micro Precision': precision_score(y_true, y_pred, average='micro'),\n",
    "        'Micro Recall': recall_score(y_true, y_pred, average='micro')\n",
    "    }\n",
    "    \n",
    "    print(f'\\nMetrics for {dataset_name}:')\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f'{metric_name}: {value:.4f}')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_pred = model.predict(X_val)\n",
    "val_metrics = evaluate_predictions(y_val, val_pred, 'Validation Set')\n",
    "\n",
    "# Evaluate on test set\n",
    "test_pred = model.predict(X_test)\n",
    "test_metrics = evaluate_predictions(y_test, test_pred, 'Test Set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6764be40",
   "metadata": {},
   "source": [
    "## 8. Visualize Results\n",
    "\n",
    "Let's create a visualization of our metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702cf8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison plot\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Validation': val_metrics,\n",
    "    'Test': test_metrics\n",
    "}).reset_index()\n",
    "metrics_df.columns = ['Metric', 'Validation', 'Test']\n",
    "\n",
    "# Melt the dataframe for easier plotting\n",
    "melted_df = pd.melt(metrics_df, id_vars=['Metric'], var_name='Dataset', value_name='Score')\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=melted_df, x='Metric', y='Score', hue='Dataset')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Model Performance Metrics')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e625e6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This improved implementation uses DistilBERT, a lightweight and efficient transformer model, for multi-label text classification on the RCV1 dataset. The implementation includes:\n",
    "\n",
    "1. Efficient batching and data loading with PyTorch\n",
    "2. Modern transformer architecture (DistilBERT) fine-tuning\n",
    "3. Proper handling of multi-label classification using BCE loss\n",
    "4. GPU acceleration when available\n",
    "\n",
    "Key advantages of this approach:\n",
    "1. Better semantic understanding of text through pre-trained language model\n",
    "2. Ability to handle out-of-vocabulary words\n",
    "3. Context-aware representations\n",
    "4. State-of-the-art performance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "6. Use cross-validation for more robust evaluation5. Add regularization techniques4. Experiment with different pooling strategies3. Use learning rate scheduling2. Implement gradient accumulation for larger batch sizes1. Try different transformer models (BERT, RoBERTa, XLNet)To further improve the results, you could:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f'{metric_name}: {value:.4f}')for metric_name, value in test_metrics.items():print('\\nTest Metrics:')test_metrics = evaluate_bert_predictions(model, test_dataloader, device)# Evaluate on test set)    shuffle=False    batch_size=batch_size,    test_dataset,test_dataloader = DataLoader()    tokenizer=tokenizer    labels=y_test.toarray(),    texts=[' '.join(rcv1.target_names[i] for i in doc.nonzero()[1]) for doc in X_test],test_dataset = RCV1Dataset(# Create test dataset and dataloader    print(f'{metric_name}: {value:.4f}')for metric_name, value in val_metrics.items():print('\\nValidation Metrics:')val_metrics = evaluate_bert_predictions(model, val_dataloader, device)print('\\nEvaluating BERT model...')# Evaluate on validation set    return metrics        }        'Micro Recall': recall_score(all_labels, all_predictions, average='micro')        'Micro Precision': precision_score(all_labels, all_predictions, average='micro'),        'Samples F1': f1_score(all_labels, all_predictions, average='samples'),\n",
    "\n",
    "\n",
    "        'Weighted F1': f1_score(all_labels, all_predictions, average='weighted'),        'Macro F1': f1_score(all_labels, all_predictions, average='macro'),plt.show()plt.tight_layout()plt.title('BERT Model Performance Metrics')plt.xticks(rotation=45)sns.barplot(data=melted_df, x='Metric', y='Score', hue='Dataset')plt.figure(figsize=(12, 6))\n",
    "\n",
    "\n",
    "        'Micro F1': f1_score(all_labels, all_predictions, average='micro'),\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    metrics = {        all_labels = np.array(all_labels)    all_predictions = np.array(all_predictions)                all_labels.extend(labels)            all_predictions.extend(predictions)\n",
    "\n",
    "                        predictions = torch.sigmoid(outputs).cpu().numpy() > 0.5            outputs = model(input_ids=input_ids, attention_mask=attention_mask)# Create the plotmelted_df = pd.melt(metrics_df, id_vars=['Metric'], var_name='Dataset', value_name='Score')\n",
    "\n",
    "\n",
    "                        labels = batch['labels'].numpy()            attention_mask = batch['attention_mask'].to(device)# Melt the dataframe for easier plottingmetrics_df.columns = ['Metric', 'Validation', 'Test']}).reset_index()    'Test': test_metrics    'Validation': val_metrics,\n",
    "\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)        for batch in tqdm(dataloader, desc='Evaluating'):\n",
    "\n",
    "\n",
    "    with torch.no_grad():        all_labels = []    all_predictions = []metrics_df = pd.DataFrame({# Create a comparison plot\n",
    "def evaluate_bert_predictions(model, dataloader, device):\n",
    "    model.eval()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
